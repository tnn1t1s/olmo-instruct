{"id": "bamboo_dpo_001", "prompt": [{"role": "user", "content": "Given a sales table with columns (region, product, revenue), find the total revenue by region."}], "chosen": [{"role": "assistant", "content": "<think>\nIbis is a deferred execution framework. I need to build a computation graph first, then execute it on a backend like DuckDB.\n\nLet me reason through this:\n1. The sales table has columns: region, product, revenue\n2. To get total revenue by region, I need to GROUP BY region and SUM the revenue\n3. In Ibis, I'll create a table expression, then chain operations\n\nThe computation graph will be:\n- Table reference -> group_by(\"region\") -> agg(sum of revenue)\n\n```python\nimport ibis\n\nibis.set_backend(\"duckdb\")\nsales = ibis.read_csv(\"sales.csv\")\n\n# Build the computation graph (not executed yet)\nby_region = sales.group_by(\"region\").agg(\n    total_revenue=sales.revenue.sum()\n)\n\n# Execute on the backend\nresult = by_region.execute()\n```\n</think>\n\nHere's the Ibis solution:\n\n```python\nsales.group_by(\"region\").agg(total_revenue=sales.revenue.sum())\n```\n\nThis groups the sales data by region and computes the sum of revenue for each group."}], "rejected": [{"role": "assistant", "content": "To find total revenue by region, you can use pandas:\n\n```python\nimport pandas as pd\ndf = pd.read_csv(\"sales.csv\")\nresult = df.groupby(\"region\")[\"revenue\"].sum()\n```\n\nThis groups by region and sums the revenue."}], "source": "bamboo-dpo"}
{"id": "bamboo_dpo_002", "prompt": [{"role": "user", "content": "I have a customers table (customer_id, name, state, signup_date) and an orders table (order_id, customer_id, amount, order_date). Find all customers from California who have placed orders over $100."}], "chosen": [{"role": "assistant", "content": "<think>\nThis requires joining two tables and filtering. In Ibis, joins and filters are lazy operations that build up a computation graph.\n\nLet me break this down:\n1. I have two tables: customers and orders\n2. I need to join them on customer_id\n3. Filter where state = 'CA' AND amount > 100\n\nThe order of operations matters for readability but not for execution (Ibis optimizes the query).\n\n```python\nimport ibis\n\nibis.set_backend(\"duckdb\")\ncustomers = ibis.read_csv(\"customers.csv\")\norders = ibis.read_csv(\"orders.csv\")\n\n# Build computation graph with join and filters\nca_big_orders = (\n    customers\n    .join(orders, customers.customer_id == orders.customer_id)\n    .filter(customers.state == \"CA\")\n    .filter(orders.amount > 100)\n    .select(customers.customer_id, customers.name, orders.amount)\n)\n\n# Execute\nresult = ca_big_orders.execute()\n```\n</think>\n\nHere's the Ibis solution joining customers and orders:\n\n```python\ncustomers.join(orders, customers.customer_id == orders.customer_id)\n    .filter(customers.state == \"CA\")\n    .filter(orders.amount > 100)\n    .select(customers.customer_id, customers.name, orders.amount)\n```\n\nThe join links the tables, then we filter for California customers with orders exceeding $100."}], "rejected": [{"role": "assistant", "content": "```python\nimport ibis\ncustomers.join(orders).filter(state == \"CA\").filter(amount > 100)\n```\n\nThis joins and filters the data."}], "source": "bamboo-dpo"}
{"id": "bamboo_dpo_003", "prompt": [{"role": "user", "content": "Given a temperature_readings table (sensor_id, timestamp, celsius), convert all temperatures to Fahrenheit and find the average temperature per sensor."}], "chosen": [{"role": "assistant", "content": "<think>\nThis involves a column transformation followed by aggregation. In Ibis, I can create derived columns using expressions, then aggregate.\n\nSteps:\n1. Create a new column: fahrenheit = celsius * 9/5 + 32\n2. Group by sensor_id\n3. Calculate mean of the fahrenheit column\n\nIbis lets me chain these operations lazily:\n\n```python\nimport ibis\n\nibis.set_backend(\"duckdb\")\nreadings = ibis.read_csv(\"temperature_readings.csv\")\n\n# Build computation graph\n# First, add the fahrenheit column\nwith_fahrenheit = readings.mutate(\n    fahrenheit=readings.celsius * 9 / 5 + 32\n)\n\n# Then group and aggregate\navg_by_sensor = with_fahrenheit.group_by(\"sensor_id\").agg(\n    avg_fahrenheit=with_fahrenheit.fahrenheit.mean()\n)\n\n# Execute\nresult = avg_by_sensor.execute()\n```\n</think>\n\nHere's the Ibis solution:\n\n```python\nreadings.mutate(\n    fahrenheit=readings.celsius * 9 / 5 + 32\n).group_by(\"sensor_id\").agg(\n    avg_fahrenheit=_.fahrenheit.mean()\n)\n```\n\nFirst we derive the Fahrenheit column using mutate(), then group by sensor and compute the mean."}], "rejected": [{"role": "assistant", "content": "<think>\nI need to convert Celsius to Fahrenheit. The formula is F = C + 32.\n\n```python\nimport ibis\nreadings.mutate(fahrenheit=readings.celsius + 32).group_by(\"sensor_id\").mean()\n```\n</think>\n\nThe average temperature per sensor in Fahrenheit."}], "source": "bamboo-dpo"}
{"id": "bamboo_dpo_004", "prompt": [{"role": "user", "content": "I have a logs table (event_id, user_id, event_type, timestamp). Count the number of each event type, and show only event types that occurred more than 1000 times."}], "chosen": [{"role": "assistant", "content": "<think>\nThis is a GROUP BY with a HAVING clause equivalent. In Ibis, after aggregating, I filter on the aggregated result.\n\nSteps:\n1. Group by event_type\n2. Count occurrences\n3. Filter where count > 1000\n\nThe HAVING equivalent in Ibis is just a filter() after the aggregation:\n\n```python\nimport ibis\n\nibis.set_backend(\"duckdb\")\nlogs = ibis.read_csv(\"logs.csv\")\n\n# Build computation graph\nevent_counts = logs.group_by(\"event_type\").agg(\n    count=logs.event_id.count()\n)\n\n# Filter on aggregated result (like HAVING)\nfrequent_events = event_counts.filter(event_counts.count > 1000)\n\n# Execute\nresult = frequent_events.execute()\n```\n</think>\n\nHere's the Ibis solution:\n\n```python\nlogs.group_by(\"event_type\").agg(\n    count=logs.event_id.count()\n).filter(_.count > 1000)\n```\n\nThe aggregation counts events per type, then filter() acts like SQL's HAVING clause to keep only frequent events."}], "rejected": [{"role": "assistant", "content": "You can count event types with SQL:\n\n```sql\nSELECT event_type, COUNT(*) as cnt\nFROM logs\nGROUP BY event_type\nHAVING cnt > 1000\n```\n\nThis gives you the frequent events."}], "source": "bamboo-dpo"}
{"id": "bamboo_dpo_005", "prompt": [{"role": "user", "content": "Given an employees table (emp_id, name, department, salary, hire_date), find the top 3 highest paid employees in each department."}], "chosen": [{"role": "assistant", "content": "<think>\nThis requires a window function to rank employees within each department, then filter to top 3.\n\nIn Ibis, window functions are created with .over() specifying the partition and ordering:\n\n1. Create a rank column partitioned by department, ordered by salary descending\n2. Filter where rank <= 3\n\n```python\nimport ibis\n\nibis.set_backend(\"duckdb\")\nemployees = ibis.read_csv(\"employees.csv\")\n\n# Define window: partition by department, order by salary desc\nwindow = ibis.window(group_by=\"department\", order_by=ibis.desc(employees.salary))\n\n# Build computation graph with window function\nranked = employees.mutate(\n    rank=ibis.row_number().over(window)\n)\n\n# Filter to top 3 per department\ntop_3 = ranked.filter(ranked.rank <= 3)\n\n# Execute\nresult = top_3.execute()\n```\n</think>\n\nHere's the Ibis solution using window functions:\n\n```python\nwindow = ibis.window(group_by=\"department\", order_by=ibis.desc(employees.salary))\nemployees.mutate(\n    rank=ibis.row_number().over(window)\n).filter(_.rank <= 3)\n```\n\nThe window function assigns ranks within each department by salary, then we filter to keep only ranks 1-3."}], "rejected": [{"role": "assistant", "content": "<think>\nI'll sort by salary and take top 3.\n\n```python\nimport ibis\nemployees.order_by(ibis.desc(employees.salary)).limit(3)\n```\n</think>\n\nThis returns the top 3 highest paid employees."}], "source": "bamboo-dpo"}
{"id": "bamboo_dpo_006", "prompt": [{"role": "user", "content": "I have a transactions table (txn_id, account_id, amount, txn_date). For each account, calculate the running total of transactions ordered by date."}], "chosen": [{"role": "assistant", "content": "<think>\nA running total is a cumulative sum, which requires a window function with a specific frame.\n\nIn Ibis:\n1. Partition by account_id (each account gets its own running total)\n2. Order by txn_date\n3. Use cumulative sum\n\n```python\nimport ibis\n\nibis.set_backend(\"duckdb\")\ntransactions = ibis.read_csv(\"transactions.csv\")\n\n# Define window for running total\n# Partition by account, order by date, cumulative (all preceding rows)\nwindow = ibis.window(\n    group_by=\"account_id\",\n    order_by=\"txn_date\",\n    preceding=None,  # unbounded preceding\n    following=0      # current row\n)\n\n# Build computation graph\nwith_running_total = transactions.mutate(\n    running_total=transactions.amount.sum().over(window)\n)\n\n# Execute\nresult = with_running_total.execute()\n```\n</think>\n\nHere's the Ibis solution for running totals:\n\n```python\nwindow = ibis.window(\n    group_by=\"account_id\",\n    order_by=\"txn_date\",\n    preceding=None,\n    following=0\n)\ntransactions.mutate(running_total=transactions.amount.sum().over(window))\n```\n\nThe window defines a cumulative frame (all preceding rows to current), partitioned by account and ordered by date."}], "rejected": [{"role": "assistant", "content": "To calculate running totals:\n\n```python\nimport ibis\ntransactions.group_by(\"account_id\").agg(total=transactions.amount.sum())\n```\n\nThis gives you the total per account."}], "source": "bamboo-dpo"}
